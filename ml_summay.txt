Lecture 1:
	1. Linear Regression
		a. LMS (Least Mean Square) algorithm. gradient descent algorithm; 
			i. batch gradient descent algorithm(Go through the entire trainning set for each update)
			ii. stochastic gradient descent, incremental gradient descent ( update parameter each time we encounter a trainning example). This is preferred when trainning set is large.
				This parameter may oscillate around the true minimum. One other way is to lower the learning rate to zero as the algorithm runs
		
		b. Normal equation (solve the minimum explicitly, quadratic equation, matrix derivative)
		
		c. OLS probablistic interpretation, assume error term Gaussian, we could write down the likelihood function for each observation, 
			maximize log likelihood function can be re-written as to minimize least square cost function
			
		d. Local weighted linear regression. Add weight term to the cost function for each observation. Weight could be determined by different kernels. Common 
			one is exponential. Think of this as a sum of multiple linear regression. Each regression focus on a sub section of the sample. If bandwidth parameter (tau)
			is lower, for each regression, the weight is lower for those further away from the x in question, so we would have a better fit locally, overall better fit 
			in the trainning example. (Be careful about overfit though)
	
	2. Classification and Logistic Regression
		a. logistic function / singmoid function. Maximize likelihood to estimate the parameter. The form of the stochastic gradient ascent update rule is identical 
		with linear regression but not the hypothesis function
		b. Digression, the perceptron learning algorithm. Replace the logistic function with a step function
		c. Fisher scoring: use Newton-Raphson to maximize the likelihood function for logistic regression. It's usually faster than batch gradient ascent if number of regressor not large
		
	3. Generalized Linear Models
		a. The exponential family distribution: p(y; eta) = b(y)exp(eta^T T(y) - a(eta)). Any distribution can be written as the form like that is a exponential family. (Gaussian, Bernoulli, etc)
		b. OLS y|x;theta ~ N(miu, sig^2); Logistic Regression y|x;theta ~ Bernoulli(phi) 
		c. GLM is a flexible generalization of ordinary linear regression that allows for response variable that have error distribution models other than normal distribution.
		d. GLM assumptions.
			i. y | x; theta ~ ExponentialFamily(eta)
			ii. prediction h(x) = E(y|x)
			iii. eta = theta^T x
		e. OLS, Logistic Regression, Softmax Regression (Not quite certain) are GLM.
		

Lecture 2:
	1. Generative learning algorithm: algo that models p(x|y) and p(y); instead of model P(y|x) directly which is discriminative learning algorithm
	
	2. Gaussian discriminant analysis:
		a. y ~ Bernoulli, x|y=0 ~ N(miu0, cov), x|y=1 ~ N(miu1, cov); basically model the feature distribution under each scenario using multivariate normal distribution; 
		b. MLE estimator can be calc from trainning sample
		c. GDA and logistic assumptions: GDA makes stronger assumptions (Gaussian) and more data efficient if the assumptions are correct.
	
	3. Naive Bayes (Multi-variate Bernoulli event model)
		a. discrete valued of x (test classification, size of the x vector is the size of the vocabulary) and GDA has continuous real valued x.
		b. Naive Bayes (NB) assumption: x_i are conditionally independent given y. If we model x_i directly with multinominal distribution, there would be too many parameters to estimate.
		c. The fit here is to get the probability of each scenario from the trainning set instead of the parameter of the distribution like mean vector in GDA
		d. If GDA doesn't work very well, one could try to discretize the x to use multinominal
		e. Laplace smoothing:
			i. this is to avoid the 0 probability estimate if some x were not seen in the trainning sample
		f. Multinomial event model. (size of the x vector is the length of the trainning sample, x_i contains the location of the word in the dictionary). This is a multinominal distribution
		g. side note: Bernoulli distribution describte single trial of a binary result, while binomial distribution is the sum of multiple Bernoulli variable. 
			Same as Multinoulli distribution and Multinomial distribution but the results are not binary any more.
			