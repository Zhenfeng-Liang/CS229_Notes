Lecture 1:
	1. Linear Regression
		a. LMS (Least Mean Square) algorithm. gradient descent algorithm; 
			i. batch gradient descent algorithm(Go through the entire trainning set for each update)
			ii. stochastic gradient descent, incremental gradient descent ( update parameter each time we encounter a trainning example). This is preferred when trainning set is large.
				This parameter may oscillate around the true minimum. One other way is to lower the learning rate to zero as the algorithm runs
		
		b. Normal equation (solve the minimum explicitly, quadratic equation, matrix derivative)
		
		c. OLS probablistic interpretation, assume error term Gaussian, we could write down the likelihood function for each observation, 
			maximize log likelihood function can be re-written as to minimize least square cost function
			
		d. Local weighted linear regression. Add weight term to the cost function for each observation. Weight could be determined by different kernels. Common 
			one is exponential. Think of this as a sum of multiple linear regression. Each regression focus on a sub section of the sample. If bandwidth parameter (tau)
			is lower, for each regression, the weight is lower for those further away from the x in question, so we would have a better fit locally, overall better fit 
			in the trainning example. (Be careful about overfit though)
	
	2. Classification and Logistic Regression
		a. logistic function / singmoid function. Maximize likelihood to estimate the parameter. The form of the stochastic gradient ascent update rule is identical 
		with linear regression but not the hypothesis function
		b. Digression, the perceptron learning algorithm. Replace the logistic function with a step function
		c. Fisher scoring: use Newton-Raphson to maximize the likelihood function for logistic regression. It's usually faster than batch gradient ascent if number of regressor not large
		
	3. Generalized Linear Models
		a. The exponential family distribution: p(y; eta) = b(y)exp(eta^T T(y) - a(eta)). Any distribution can be written as the form like that is a exponential family. (Gaussian, Bernoulli, etc)
		b. OLS y|x;theta ~ N(miu, sig^2); Logistic Regression y|x;theta ~ Bernoulli(phi) 
		c. GLM is a flexible generalization of ordinary linear regression that allows for response variable that have error distribution models other than normal distribution.
		d. GLM assumptions.
			i. y | x; theta ~ ExponentialFamily(eta)
			ii. prediction h(x) = E(y|x)
			iii. eta = theta^T x
		e. OLS, Logistic Regression, Softmax Regression (Not quite certain) are GLM.
		

Lecture 2:
	1. Generative learning algorithm: algo that models p(x|y) and p(y); instead of model P(y|x) directly which is discriminative learning algorithm
	
	2. Gaussian discriminant analysis:
		a. y ~ Bernoulli, x|y=0 ~ N(miu0, cov), x|y=1 ~ N(miu1, cov); basically model the feature distribution under each scenario using multivariate normal distribution; 
		b. MLE estimator can be calc from trainning sample
		c. GDA and logistic assumptions: GDA makes stronger assumptions (Gaussian) and more data efficient if the assumptions are correct.
	
	3. Naive Bayes (Multi-variate Bernoulli event model)
		a. discrete valued of x (test classification, size of the x vector is the size of the vocabulary) and GDA has continuous real valued x.
		b. Naive Bayes (NB) assumption: x_i are conditionally independent given y. If we model x_i directly with multinominal distribution, there would be too many parameters to estimate.
		c. The fit here is to get the probability of each scenario from the trainning set instead of the parameter of the distribution like mean vector in GDA
		d. If GDA doesn't work very well, one could try to discretize the x to use multinominal
		e. Laplace smoothing:
			i. this is to avoid the 0 probability estimate if some x were not seen in the trainning sample
		f. Multinomial event model. (size of the x vector is the length of the trainning sample, x_i contains the location of the word in the dictionary). This is a multinominal distribution
		g. side note: Bernoulli distribution describte single trial of a binary result, while binomial distribution is the sum of multiple Bernoulli variable. 
			Same as Multinoulli distribution and Multinomial distribution but the results are not binary any more.
			
Lecture 3:
	1. Margins:
		a. Functional margin r(i) = y(y) (w^Tx +b)
		b. Geometric margin: the scale of the functional margin doesn't matter. Geometric margin = functional margin / norm of w
		c. The optimal margin classifier is to maximize the margin. (optimization problem). With some transformation tricks, this will be turned into a problem can be solved with KKT
	2. Optimal margin classifiers:
		a. Due to complementary slackness of KKT conditions, only the alpha on the support vectors would be non-zero. The predictions can be written in terms of the trainning data
	3. Kernel:
		a. Feature mapping is to convert the original input attributes into a higher dimensional space. 
		b. Kernel function can be written as dot product of the feature mapping
		c. kernel function is usually much less inexpensive to compute compared to explicitly compute the high dimensional feature mapping. Reduce complexity
		d. Mercer theorem: Kernel function is valid i.f.f the kernel matrix is symmetric positive semi-definite matrix
		e. Idea: through feature mapping, we can fit non-linear more complex problem but introduced high dimensional computation problems. Kernel can help reduce the computation cost.
		
	4. Regularization:
		a. Put in regularization can give SVM a tolerance to those outliers. Not to classify everything 100% in the trainning sample but keeping a large margin.
		
	5. SMO (sequential minimal optimization) algorithm:
		a. for each parameter, use coordinate ascent to find the parameter to maximize the objective function. Loop until it converge.
	
	
Lecture 4: Learning Theory
	1. Bias and variance trade off. 
	2. Empirical risk minimization (ERM) minimize the trainning error
	3. For finite H, the trainning sample size needed is linear to log k, where k is the number of hypothesis, given margin and confidence level.
	4. For infinite H, the trainning sample size needed is linear to d, where d is the number of parameter given the margin and confidence level.
	5. Vapnik-Chernonenkis (VC(H)) dimension, is the size of the largest set that is shattered by H
	
	
Lecture 5: (Regularization and model selection)
	1. hold-out cross validation (simple cross validation), one cut cross validation, usually 70:30.
	2. k-fold cross validation: cut data into k-fold, every time use one of those as validation set, so run the cross validation k times and take the average
		of the generalization error to pick the model. (leave-one-out cross validation: each cut of sample is just one observation)
	3. Feature selection:
		a. Forward search: starting from zero feature, keep adding new best feature if the updated model can reduce the cross validation generalization error. 
			The algo basically adds the features in the order of it's usefulness.
		b. Backward search: starting from the full set of features, and deleting the feature in the order of its uselessness.
		c. Above are computation expensive. So we can use filter feature selection. Mutual information (MI) is a kind of measure about the dependency between the label and the feature.
			Sorted the feature in the order of the MI. With the ranks, use cross validation to select the most important features...
			
Lecture 6:
	1. The perceptron and large margin classifiers. Online learning: predict while trained. The number of mistakes that the perceptron algorithm makes on this sequence is 
		bounded by a function of margin and maximum x norm.
		
		
Lecture 7a: (k-means clustering algorithm)
	1. select intial cluster centroids (miu_i)
		repeat until convergence: {
			for each sample point, group to the closest cluster centroid
			
			for each group, recalculate the cluster centroid with the averge of the sample in the group
		}
	2. distortion function, J(c, miu) = sum(||x_i - miu_(c_i)||^2) for all the samples. The algorithm is basically running the coordinate descent on J, 
		the first loop in to minimize J with respect to c and the second loop is to minimize J with respect to miu 


Lecture 7b: (Mixtures of Gaussians and the EM algorithm)
	1. Mixtures of Gaussian: for each sample x_i, it could be generated randomly from one of the k Gaussian distributions. The latent variable z will also have a distribution.
	2. l (log likelihood) = sum(log p) = sum(log sum (p(x_i|z_j) * p(z_j)))
	3. EM (Expectation-Maximization) algorithm. In the E-step, it "guess" the value of z_j, and in the M-step, it runs the maximization based on the guess. This runs for each sample 
		and each possible Gaussian guess. Repeat until convergence.


Lecture 8: (The EM algorithm)
	1. Jensen's inequality: for convex function, E[f(x)] >= f(EX). The equality only holds true if and only if X=E[X] with probability 1, i.e. if X is a constant.
	2. The idea is find the lower bound of the log likelihood function using Jensen's inequality of concave function. And then find the condition to make the equality 
		hold true, i.e. the variable is constant. Then you could write down the "guess" of the distribution with probability. Then the maximization step can be solved 
		by finding theta. EM algo can be viewed as coordinate ascent on J, E-step maximize it with respect to Q and M maximize it with respect to theta.
	3. To find the fit of each Gaussian in the mixture Gaussian, we just need to take the derivative of the maximization objective function with respect to each one of the variable,
		and set them to zero. With some algebra, we can have a very clean form.
