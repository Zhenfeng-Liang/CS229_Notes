Lecture 1:
	1. Linear Regression
		a. LMS (Least Mean Square) algorithm. gradient descent algorithm; 
			i. batch gradient descent algorithm(Go through the entire trainning set for each update)
			ii. stochastic gradient descent, incremental gradient descent ( update parameter each time we encounter a trainning example). This is preferred when trainning set is large.
				This parameter may oscillate around the true minimum. One other way is to lower the learning rate to zero as the algorithm runs
		
		b. Normal equation (solve the minimum explicitly, quadratic equation, matrix derivative)
		
		c. OLS probablistic interpretation, assume error term Gaussian, we could write down the likelihood function for each observation, 
			maximize log likelihood function can be re-written as to minimize least square cost function
			
		d. Local weighted linear regression. Add weight term to the cost function for each observation. Weight could be determined by different kernels. Common 
			one is exponential. Think of this as a sum of multiple linear regression. Each regression focus on a sub section of the sample. If bandwidth parameter (tau)
			is lower, for each regression, the weight is lower for those further away from the x in question, so we would have a better fit locally, overall better fit 
			in the trainning example. (Be careful about overfit though)
	
	2. Classification and Logistic Regression
		a. logistic function / singmoid function. Maximize likelihood to estimate the parameter. The form of the stochastic gradient ascent update rule is identical 
		with linear regression but not the hypothesis function
		b. Digression, the perceptron learning algorithm. Replace the logistic function with a step function
		c. Fisher scoring: use Newton-Raphson to maximize the likelihood function for logistic regression. It's usually faster than batch gradient ascent if number of regressor not large
		
	3. Generalized Linear Models
		a. The exponential family distribution: p(y; eta) = b(y)exp(eta^T T(y) - a(eta)). Any distribution can be written as the form like that is a exponential family. (Gaussian, Bernoulli, etc)
		b. OLS y|x;theta ~ N(miu, sig^2); Logistic Regression y|x;theta ~ Bernoulli(phi) 
		c. GLM is a flexible generalization of ordinary linear regression that allows for response variable that have error distribution models other than normal distribution.
		d. GLM assumptions.
			i. y | x; theta ~ ExponentialFamily(eta)
			ii. prediction h(x) = E(y|x)
			iii. eta = theta^T x
		e. OLS, Logistic Regression, Softmax Regression (Not quite certain) are GLM.
		
		